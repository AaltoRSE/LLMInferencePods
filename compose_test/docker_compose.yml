version: "3.9"
services:
  backend:
    build:
      dockerfile: ../FastChat/Dockerfile
      context: ../FastChat
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - MODEL=lmsys/fastchat-t5-3b-v1.0
    container_name: inference-server
    volumes:
      - /home/thomas/.cache/huggingface/:/huggingface
  nginx:
    build:
      dockerfile: ../nginxPod/Dockerfile
      context: ../nginxPod
    container_name: nginx
    environment:
      - LLM_MODEL=fastchat-t5-3b-v1.0
      - AUTH_TOKEN=123
    ports:
      - 127.0.0.1:80:80
      - 127.0.0.1:443:443
networks:
  default:
    name: inference-network
