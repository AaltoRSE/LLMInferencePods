# This file is an nginx config file that can be used in combination with API servers. 
# It is currently set up for use with FastChat and allows testing, whether the FastChat server is fully loaded.


server {        

        listen 80;
        server_name localhost;

        location /alive {
            # Adapt to whatever end-point you use.            
            proxy_pass http://inference-server:8000;
            
            # Use Lua to inspect response and modify if necessary
            content_by_lua_block {
                local res = ngx.location.capture("/v1/models")                                
                local model = os.getenv("LLM_MODEL")
                if res.status == 200 and string.find(res.body, model, 1, true) then            
                    ngx.status = 200
                    ngx.header.content_type = "application/json"
                    ngx.print('{"status": "ready"}')
                    return ngx.exit(ngx.HTTP_OK)
                else
                    ngx.status = ngx.HTTP_BAD_REQUEST
                    ngx.header.content_type = "text/plain; charset=utf-8"
                    ngx.say("Bad request! Model not loaded yet: ", model)                    
                    return ngx.exit(ngx.HTTP_BAD_REQUEST)
                end
            }
        }

        location / {            
            access_by_lua_block {   
                -- Define the expected API key value
                local expected_api_key = os.getenv("AUTH_TOKEN")
                
                -- Retrieve the value of the X-API-Key header
                local provided_api_key = ngx.var.http_x_apikey
                
                -- Check if the provided API key matches the expected one
                if provided_api_key ~= expected_api_key then                
                    ngx.status = ngx.HTTP_FORBIDDEN 
                    ngx.say("Access Forbidden. Invalid or missing API key.")
                    return ngx.exit(ngx.HTTP_FORBIDDEN)                
                end
            }
            proxy_pass http://inference-server:8000;            
        }
    }