# Use the image as specified
FROM nvidia/cuda:12.5.0-devel-ubuntu22.04 

# Update and upgrade the existing packages 
RUN apt-get update && apt-get upgrade -y && apt-get install -y --no-install-recommends \
    git build-essential \
    python3 python3-pip gcc wget \
    ocl-icd-opencl-dev opencl-headers clinfo \
    libclblast-dev libopenblas-dev \
    ninja-build \        
    pkg-config \
    git \
    && mkdir -p /etc/OpenCL/vendors && echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd

ENV CUDA_DOCKER_ARCH=all
ENV GGML_CUDA=1

# Install requisites
RUN python3 -m pip install --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context

WORKDIR /llama_cpp

RUN git clone https://github.com/tpfau/llama-cpp-python.git && cd llama-cpp-python && git checkout stream_testing && git submodule init && git submodule update

RUN CMAKE_ARGS="-DGGML_CUDA=on" pip install /llama_cpp/llama-cpp-python[server]

# We set some default values, but those could be overwritten
ENV HOST="localhost" \
    PORT="8000" \        
    CHATFORMAT="chatml" \
    N_CTX="2048"

WORKDIR /app

COPY start_server_gpu.sh /app/entrypoint.sh

ENTRYPOINT ["/app/entrypoint.sh"]
