apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: llm-{{ .Values.modelName }}-{{ .Values.target }}-scaledobject
  namespace: {{ .Values.namespace }}
spec:
  scaleTargetRef:
    name: llm-{{ .Values.modelName }}-{{ .Values.target }}
    kind: Deployment
    apiVersion: apps/v1
  minReplicaCount: 1
  maxReplicaCount: 10
  cooldownPeriod: 60
  pollingInterval: 5
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.k8s.aalto.fi
        metricName: DCGM_FI_DEV_GPU_UTIL
        threshold: "0.75"
        query: |
          avg_over_time(DCGM_FI_DEV_GPU_UTIL{container="llm-{{ .Values.modelName }}-{{ .Values.target }}-cont"}[60s])


# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: llm-{{ .Values.modelName }}-{{ .Values.target }}-scaler
#   namespace: rse
# spec:
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: llm-{{ .Values.modelName }}-{{ .Values.target }}
#   minReplicas: 1
#   maxReplicas: {{ .Values.maxreplicas }}
#   metrics:
#     - type: Resource
#       resource:
#         name: cpu
#         target:
#           type: Utilization
#           averageUtilization: 50
#   behavior:
#     scaleDown:
#       stabilizationWindowSeconds: 20
#       policies:
#         - type: Pods
#           value: 1
#           periodSeconds: 120
#     scaleUp:
#       stabilizationWindowSeconds: 20
#       policies:
#         - type: Pods
#           value: 1
#           periodSeconds: 20
