apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-{{ .Values.modelPath }}-{{ .Values.target }}
  namespace: {{ .Values.namespace}}
  labels:
    app.kubernetes.io/name: llm-{{ .Values.modelPath }}-{{ .Values.target }}
    app.kubernetes.io/component: server
# The definition of the service itself
spec:  
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-{{ .Values.modelPath }}
      app.kubernetes.io/component: server
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llm-{{ .Values.modelPath }}
        app.kubernetes.io/component: server
    spec:            
      restartPolicy: Always
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      volumes:
        - name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-pvc
          #TODO: This needs to be a folder that is structured like HF_HUB
        - name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-cache-pvc
        
      containers:
        - name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-cont          
          image: vllm/vllm-openai:{{ .Values.vllm_version }}
          ports:
            - containerPort: 8000  
          resources: # Restrict to one GPU
            requests:              
              cpu: "250m"
            limits:             
              nvidia.com/gpu: {{ .Values.gpus}} 
              cpu: {{ .Values.cpus }}
          env:
            - name: HF_HOME
              value: "/container-home/huggingface"
            # Remove when actually deploying
            - name: HUGGING_FACE_HUB_TOKEN
              value: "{{ .Values.HF_TOKEN}}"
            # Need to unset, so that it doesn't use more than those allocated, maybe this is sufficient 
            - name: NVIDIA_VISIBLE_DEVICES
              value: "0"
            # Deactivated for testing purposes
            #- name: TRANSFORMERS_OFFLINE 
            #  value: '1'
            - name: VLLM_CONFIG_ROOT
              value: "/container-home/config"
            - name: VLLM_LOGGING_LEVEL
              value: DEBUG
            - name: VLLM_TRACE_FUNCTION
              value: "1"
            - name: VLLM_API_KEY
              valueFrom:
                  { secretKeyRef: { name: llm-gateway, key: inference_key } }
          args:
            - --model
            - {{ .Values.modelName }}            
            - --root-path=/{{ .Values.modelPath }}            
          volumeMounts:
            - mountPath: /container-home/
              name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-pvc          
            - mountPath: /.cache
              name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-cache-pvc
          readinessProbe:
            httpGet:
              path: /{{ .Values.modelPath }}/health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 20
            timeoutSeconds: 10
            failureThreshold: 10
        


# Persistent volume claim for testing

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-pvc
  namespace: {{ .Values.namespace }}
spec:
  accessModes: 
    - ReadWriteMany # We need to be able to access this from multiple pods..
  resources:
    requests:
      # Need to have a look how big this will become.
      storage: {{ .Values.modelsize }}
  storageClassName: moodle-nfs-csi
  volumeMode: Filesystem

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-{{ .Values.modelPath }}-{{ .Values.target }}-cache-pvc
  namespace: {{ .Values.namespace }}
spec:
  accessModes: 
    - ReadWriteMany # We need to be able to access this from multiple pods..
  resources:
    requests:
      # Need to have a look how big this will become.
      storage: {{ .Values.modelsize }}
  storageClassName: moodle-nfs-csi
  volumeMode: Filesystem
#---
